+++
title = "Data Infrastructure Gaps Undermine AI Success, Hylaine Executive Warns"
date = "2025-10-17T18:00:31Z"
draft = false
summary = "Inadequate data preparation represents the primary failure point for AI initiatives, with structural, technical and organizational barriers requiring investment in mature data infrastructure and governance frameworks."
description = "Hylaine VP Ryan McElroy reveals 5 critical AI data readiness roadblocks and solutions. Learn how to build sustainable AI infrastructure and avoid common implementation failures."
source_link = "https://www.citybiz.co/article/760088/qa-with-ryan-mcelroy-vice-president-of-technology-of-hylaine-solving-the-ai-data-readiness-problem/"
enclosure = "https://cdn.newsramp.app/citybiz/newsimage/01ddcc4c6ac64e5bcccba8d55b9ca825.jpg"
article_id = 257383
feed_item_id = 22550
qrcode = "https://cdn.newsramp.app/citybiz/qrcode/2510/17/pend0QPf.webp"
source = "citybiz"
+++

<p>Many AI initiatives fail not because of flawed algorithms but due to inadequate data preparation, according to Ryan McElroy, Vice President of Technology at Hylaine. The biggest barriers for preparing AI data are structural, technical, and organizational, with challenges consistently appearing in five key areas: data access, siloed systems, data quality, governance, and the human factor.</p><p>Data access issues often stem from information that exists but cannot be used due to legal or security blocks, or because it is housed in incompatible formats or legacy systems. Siloed data remains a long-standing problem, especially as enterprises spread operations across multiple cloud platforms. Even when data can be accessed, quality problems like inaccuracies, redundancies, and incomplete records undermine model accuracy and lead to hallucinations or bias.</p><p>McElroy emphasizes that tech leaders should begin by building a mature, AI-ready data infrastructure that includes investing in data engineering tools and talent. This means modernizing data architectures to handle additional ways of collecting, processing, and storing data at the scale and velocity AI requires. Companies that have both data warehouses with curated, reliable, and structured data sets and data lakes built to accommodate diverse data types have a head start.</p><p>In parallel, leaders should establish data reliability engineering as a core capability in the data organization to ensure ongoing data quality, availability, and observability. These same capabilities streamline testing and root cause analysis when errors occur in data movement. Once basic infrastructure and high quality architecture is in place, organizations can adopt modern tools for data integration such as highly managed ELT tools including <a href="https://fivetran.com" rel="nofollow" target="_blank">FiveTran</a> or <a href="https://airbyte.com" rel="nofollow" target="_blank">Airbyte</a>, or cloud-native ETL platforms like Azure Data Factory or Databricks.</p><p>Success stories from companies like American Express and Astra Zeneca demonstrate that investment in robust, AI-ready data architecture builds the foundation for reliable, repeatable processes to develop AI systems that retain feedback, remember context, and improve over time. American Express built a system capable of analyzing transactions from millions of cardholders and merchants in real time to spot patterns of potential fraud because their data architecture could support continuous learning and feedback loops.</p><p>Building trust in AI systems requires transparency, explainability, and collaboration between IT and business teams. McElroy notes that the most successful AI projects are led by a trio of champions: an executive sponsor, the business process owner, and a technical lead. Together, they ensure alignment across strategy, outcomes, and execution. For organizations just starting out, selecting user groups that are both already pro-AI and vocal as initial targets for projects can reduce risk and ensure that feedback comes quickly.</p><p>Strong governance frameworks are essential for both protecting the business and accelerating AI innovation. A good governance framework defines clear rules for data use, protects personal data, and prevents unauthorized use of proprietary content or data. McElroy recommends creating a governance council that includes representatives from different parts of the business, as well as IT experts. To speed AI innovation, organizations can implement technical safeguards such as tokenizing real data and automating alerts for PII exposure using tools like <a href="https://perforce.com" rel="nofollow" target="_blank">Perforce's Delphi</a> to support continuous compliance without slowing development.</p><p>Ultimately, technology alone does not deliver ROI people do. Many AI projects falter because the data teams responsible for implementation lack experience with modern cloud infrastructure, data engineering, and DevOps. Companies can close this gap by creating hybrid teams that pair internal staff with external experts, allowing teams to learn new ways of working with data that they rarely want to revert from once mastered.</p>